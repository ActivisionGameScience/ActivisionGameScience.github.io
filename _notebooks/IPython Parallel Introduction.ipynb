{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Deploy an IPython Cluster Using Mesos and Docker\n",
    "\n",
    "[John Dennison](https://github.com/jofusa)\n",
    "\n",
    "April 19th, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The members of the Analytics Services team here at Activision are heavy users of [Mesos](http://mesos.apache.org/) and [Marathon](https://github.com/mesosphere/marathon) to deploy and manage services on our clusters. We are also huge fans of Python and the [Jupyter](http://jupyter.org/) project. \n",
    "\n",
    "The Jupyter project was recently reorganized from IPython, in a move referred to as \"the split\": One part that was originally part of IPython (`IPython.parallel`) was split off into a separate project [ipyparallel](https://github.com/ipython/ipyparallel). This powerful component of the IPython ecosystem is generally overlooked.\n",
    "\n",
    "In this post I will give a quick introduction to the ipyparallel project and then introduce a new launcher we have open sourced to deploy IPython clusters into Mesos clusters. While we have published this notebook in HTML, please feel free to download the [original](https://github.com/ActivisionGameScience/ActivisionGameScience.github.io/blob/master/_notebooks/IPython%20Parallel%20Introduction.ipynb) to follow along.\n",
    "\n",
    "## Introduction to ipyparallel\n",
    "\n",
    "The ipyparallel project is the new home of IPython.parallel module that was hosted within IPython core before 2015. The focus of the project is interactive cluster computing. This focus on interactive computing and first-class integration with the IPython project is a distinguishing feature. For a more complete dive into the internals of ipyparallel, please visit the [docs](https://ipyparallel.readthedocs.org/en/latest/intro.html). I aim to give the bare minimum to get you started.\n",
    "\n",
    "At the most basic level an IPython cluster is a set of Python interpreters that can be accessed over TCP. Under the hood, it works similarly to how Jupyter/IPython work today. When you open a new notebook in the browser, a Python process (called a kernel) will be started to run the code you submit. ipyparallel does the same thing except instead of a single Python kernel, you can start many distributed kernels over many machines.\n",
    "\n",
    "There are three main components to the stack. \n",
    "- Client: A Python process which submits work. Usually this is an IPython session or a Jupyter notebook. \n",
    "- Controller: The central coordinator which accepts work from the client and passes it to engines, collects results and sends back to the client.\n",
    "- Engine: A Python interpreter that communicates with the controller to accept work and submit results. Roughly equivalent to an IPython kernel. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting your first cluster\n",
    "\n",
    "The easiest way to get your hands dirty is to spin up a cluster locally. That is you will run a Client, Controller, and Engines all on your local machine. The hardest part of provisioning distributed clusters is making sure all the pieces can talk to each other (as usual the easiest solution to a distributed problem is to make it local).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your environment started\n",
    "\n",
    "Our team are users of [conda](http://conda.pydata.org/) to help manage our computational environments (Python and beyond). Here is a quick run through to get setup (our public conda recipes are [here](https://github.com/ActivisionGameScience/ags_conda_recipes)). A combination of pip and virtualenv will also work, but when you start installing packages from the scipy stack we find conda the easiest to use.\n",
    "\n",
    "First find your version of Miniconda from [here](http://conda.pydata.org/miniconda.html)\n",
    "\n",
    "If you're using linux these commands will work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "wget https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n",
    "bash Miniconda-latest-Linux-x86_64.sh # follow prompts\n",
    "conda update --all\n",
    "# make a new python 3 env named py3\n",
    "conda create -n py3 python=3 ipython ipyparallel ipython-notebook\n",
    "source activate py3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are lower level commands to start and configure Controllers and Engines, the primary command you will use is `ipcluster`. This is a helpful utility to start all the components and configure your local client. By default, it uses the `LocalControllerLauncher` and the `LocalEngineSetLauncher` which is exactly what we want to start. \n",
    "\n",
    "Open a terminal install `ipyparallel` and start a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "(py3)➜ ipcluster start --n=4\n",
    "2016-04-11 22:24:15.514 [IPClusterStart] Starting ipcluster with [daemon=False]\n",
    "2016-04-11 22:24:15.515 [IPClusterStart] Creating pid file: /home/vagrant/.ipython/profile_default/pid/ipcluster.pid\n",
    "2016-04-11 22:24:15.515 [IPClusterStart] Starting Controller with LocalControllerLauncher\n",
    "2016-04-11 22:24:16.519 [IPClusterStart] Starting 2 Engines with LocalEngineSetLauncher\n",
    "2016-04-11 22:24:46.633 [IPClusterStart] Engines appear to have started successfully\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can also use the IPython magic shell command. but errors are harder to see and stopping the cluster can be janky.\n",
    "!ipcluster start -n 4 --daemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If started correctly we should now have four engines running on our local machine. Now to actually interact with them. First we need to import the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.ids # list the ids of the engine the client can communicate with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The client has two primary way to farm out work to the engines. First is a direct view. This is used to apply the same work to all engines. To create a `DirectView` just slice the client.\n",
    "\n",
    "The second way is a `LoadBalancedView` which we will cover later in the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DirectView [0, 1, 2, 3]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = rc[:]\n",
    "dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a direct view you can issue a function to execute within the context of that engine's Python process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31183, 31184, 31186, 31188]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_engine_pid():\n",
    "    import os\n",
    "    return os.getpid()\n",
    "    \n",
    "dv.apply_sync(get_engine_pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern is so common that ipyparallel provides a IPython magic function to execute a code cell to all engines: `%%px`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:9]: \u001b[0m31183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:9]: \u001b[0m31184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:9]: \u001b[0m31186"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:9]: \u001b[0m31188"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is key to notice that the engines are fully running stateful Python interpreters. If you set a varible within `%%px` code block, it will remain there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "foo = 'bar on pid {}'.format(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:11]: \u001b[0m'bar on pid 31183'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:11]: \u001b[0m'bar on pid 31184'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:11]: \u001b[0m'bar on pid 31186'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:11]: \u001b[0m'bar on pid 31188'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DirectView` object provides some syntactic sugar to help distributing data to each engine. First is dictionary style retrieval and assignment. First let's retrieve the value of `foo` from each engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bar on pid 31183',\n",
       " 'bar on pid 31184',\n",
       " 'bar on pid 31186',\n",
       " 'bar on pid 31188']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv['foo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can overwrite it's its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bar', 'bar', 'bar', 'bar']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv['foo'] = 'bar'\n",
    "dv['foo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many cases where you don't want the same data on each machine, but rather you want to chuck an list and distribute each chunk to an engine. The `DirectView` provides the `.scatter` and the `.gather` methods for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: scatter>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with a list of ids to work on\n",
    "user_ids = list(range(1000))\n",
    "dv.scatter('user_id_chunk', user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this method completed almost immediately and returned an `AsyncResult`. All the methods we have used up to now have be blocking and synchronous. The `scatter` method is aysnc. To turn this scatter into a blocking call we can chain a `.get()` to the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.scatter('user_id_chunk', user_ids).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a variable on each engine that holds an equal amount of the original list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Len 250\n",
      "Max 249\n",
      "[stdout:1] \n",
      "Len 250\n",
      "Max 499\n",
      "[stdout:2] \n",
      "Len 250\n",
      "Max 749\n",
      "[stdout:3] \n",
      "Len 250\n",
      "Max 999\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(\"Len\", len(user_id_chunk))\n",
    "print(\"Max\", max(user_id_chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a simple function to each list. First, declare a function within each engine. The `--local` flag also executes the code block in your local client. This is very useful to help debug your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "def the_most_interesting_transformation_ever(user_id):\n",
    "    \"\"\"\n",
    "    This function is really interesting\n",
    "    \"\"\"\n",
    "    return \"ID:{}\".format(user_id * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ID:3'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_most_interesting_transformation_ever(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "transformed_user_ids = list(map(the_most_interesting_transformation_ever, user_id_chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 4 separate list of transformed ids. We want to stitch the disparate lists into one list on our local notebook. `gather` is used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_transformed_user_ids = dv.gather('transformed_user_ids').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['ID:0', 'ID:3', 'ID:6', 'ID:9', 'ID:12', 'ID:15', 'ID:18', 'ID:21', 'ID:24', 'ID:27']\n"
     ]
    }
   ],
   "source": [
    "print(len(all_transformed_user_ids))\n",
    "print(all_transformed_user_ids[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this example is contrived. The serialization cost of shipping Python objects over the wire to each engine is more expensive than the calculation we performed. This tradeoff between serialization/transport vs computation cost is central to any decision to use distributed processing. However, there are many highly parallelizable problems where this project can be extremely useful. Some of the main use cases we use ipyparallel for are hyperparameter searches and bulk loading/writing from storage systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadBalancedView\n",
    "\n",
    "The previous example where you scatter a list, perform a calculation, and then gather a result works for lots of problems. One issue with this approach is that each engine does an identical amount of work. If the complexity of the process each engine is performing is variable, this naive scheduling approach can waste processing power and time. Take for example this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "import random\n",
    "import time\n",
    "def fake_external_io(url):\n",
    "    # Simulate variable complexity/latency\n",
    "    time.sleep(random.random())\n",
    "    return \"HTML for URL: {}\".format(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 795 µs, sys: 312 µs, total: 1.11 ms\n",
      "Wall time: 479 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HTML for URL: 1'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fake_external_io(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 ms, sys: 0 ns, total: 2.58 ms\n",
      "Wall time: 373 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HTML for URL: 1'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fake_external_io(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had a list of urls to scrape and gave each worker an equal share, some workers would finish early and have to sit around doing nothing. A better approach is to assign work to each engine as it finishes. This way the work will be load balanced over the cluster and you will complete your process earlier. ipyparallel provides the `LoadBalancedView` for this exact use case. For this specific problem, threading or an async event loop would likely be a better approach to speeding up or scaling out, but suspend your disbelief for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LoadBalancedView None>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lview = rc.load_balanced_view()\n",
    "lview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@lview.parallel()\n",
    "@ipp.require('time', 'random')\n",
    "def p_fake_external_io(url):\n",
    "    # Simulate variable complexity/latency\n",
    "    time.sleep(random.random())\n",
    "    return \"HTML for URL: {}\".format(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used two ipyparallel decorators. First we used `lview.parallel()` to declared this a parallel function. Second, we declared that this function depends on the modules time and random. Now that we have a load balanced function we can compare timings with our naive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = ['foo{}.com'.format(i) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 ms, sys: 17.7 ms, total: 20.8 ms\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "# Naive single threaded\n",
    "%time res = list(map(fake_external_io, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.scatter('urls', urls).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed for some semblance reproducability\n",
    "%px random.seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.5 ms, sys: 8.35 ms, total: 23.8 ms\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "# Naive aassignment\n",
    "%time %px results = list(map(fake_external_io, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 381 ms, sys: 53.3 ms, total: 434 ms\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "# Load balanced version\n",
    "%time res = p_fake_external_io.map(urls).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a perfect example, but you can get the idea. The larger the number of inputs to your parallel problem, the more variable the run time of each component process, the more time you save from switching to a load balanced view.\n",
    "\n",
    "This is only scratching the surface of ipyparallel project. I would highly recommend taking a look at the [docs](https://ipyparallel.readthedocs.org/en/latest/). Here is a list of further topics I would look into if you are interested.\n",
    "\n",
    "- Support for numpy memmap to allow engine located on a single node to share large arrays\n",
    "- Complex dependencies and more specialized scheduling \n",
    "- Retry and recovery logic\n",
    "- Multiple clients working on the same cluster allowing remote collaborators to share an environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Trivial Use Cases\n",
    "\n",
    "Our team at Activision largely uses ipython clusters for distributed model training. This project has been vital for hyperparameter searches for our machine learning models, allowing us to easily parallelize these searches beyond one machine has sped up training by many orders of magnitude utilizing hundreds of cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok this is cool but I want more cores!!!\n",
    "\n",
    "The examples so far are a useful introduction to the API and the some features of ipyparallel. Hopefully you are convinced to try out the library. However, deploying a working cluster beyond a single machine introduces some issues. \n",
    "\n",
    "ipyparallel provides support for a range of cluster and batch job management systems such as PBS and WindowsHPC. The full list is provided in the documentation. ipyparallel also provides an SSH based launcher. Given passwordless ssh onto machine you can easily deploy engines and connect them to your controller and client. Also there is a wonderful project [starcluster](http://star.mit.edu/cluster/docs/latest/plugins/ipython.html) that helps spin up machines from cloud providers. \n",
    "\n",
    "These tools are great. If you have access to existing HPC clusters or are planning on deploying dedicated clusters either on your own cold-iron (2016 version of bare-metal) or in the cloud then they meet your needs.\n",
    "\n",
    "However, we are big users of Mesos, Docker, and Marathon to manage our clusters and services. Furthermore, even with the existing launchers, managing complex dependencies within the engines is a pain. Using Docker to package all dependencies makes deploying heterogeneous clusters easier. Targeting our existing cluster management system and simplifying dependencies is a big win for us.\n",
    "\n",
    "With this in mind, we are open sourcing a new ipyparallel launcher that deploys IPython clusters into Mesos using Docker and Marathon. The code lives [here](https://github.com/ActivisionGameScience/ipyparallel-mesos/) and on pypi/conda as `ipyparallel_mesos`.\n",
    "\n",
    "We have two pre-built Docker images for the [Controller](https://hub.docker.com/r/jdennison/ipyparallel-marathon-controller/) and [Engine](https://hub.docker.com/r/jdennison/ipyparallel-marathon-engine/). These are stripped down Docker images. Internally we use conda for almost all our dependencies, even inside our Docker containers. Please visit our public [conda recipes](https://github.com/ActivisionGameScience/ags_conda_recipes) and [channel](https://anaconda.org/ActivisionGameScience). However, extending from the `ipyparallel-marathon-engine` image will allow you to easily install your custom dependencies with or without conda.\n",
    "\n",
    "The project is young, but hopefully you will find it useful. Please note that this currently targets Python 3. PR's are welcome to support older versions of Python (it's 2016, we can now refer to 2.7 as old). Please open any issues on the github page. Please read the README for the project for more details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources\n",
    "\n",
    "- [ipyparallel_mesos](https://github.com/Activision/ipyparallel-mesos/)\n",
    "- [StarCluster](http://star.mit.edu/cluster/docs/latest/plugins/ipython.html)\n",
    "- [Great IPython Parallel Course - Day 3](https://github.com/jupyter/ngcm-tutorial/tree/master/Day-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
