{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ipyparallel and deploying an IPython cluster using Mesos and Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Analytics Services team here at Activision are heavy users of [Mesos](http://mesos.apache.org/) and [Marathon](https://github.com/mesosphere/marathon) to deploy and mangage services on our clusters. We are also huge fans of Python and the jupyter project. \n",
    "\n",
    "The [jupyter](http://jupyter.org/) project recently reogorganized from IPython, in a move refered to as \"the split\". One part that was orginally part of IPython (`IPython.parallel`) that was split off into a seperate project is [ipyparallel](https://github.com/ipython/ipyparallel). This powerful component of the IPython ecosystem is generally overlooked.\n",
    "\n",
    "In this post I will give a quick introduction to the ipyparallel project and then introduce a new launcher we have open sourced to deploy IPython clusters to into Mesos clusters. While we have published this notebook in HTML, please feel free to download the [original](https://github.com/ActivisionGameScience/ActivisionGameScience.github.io/blob/master/_notebooks/IPython%20Parallel%20Introduction.ipynb) to follow along.\n",
    "\n",
    "## Introduction to ipyparallel\n",
    "\n",
    "The ipyparallel project is the new home of IPython.parallel module that was hosted within IPython core before 2015. The focus of the project is interactive cluster computing. I believe this focus on interactive computing and first class intergration with the IPython project is a distingishing feature. For a more complete dive into the internals of ipyparallel please visit the [docs](https://ipyparallel.readthedocs.org/en/latest/intro.html). I aim to give the bare minimum to get you started.\n",
    "\n",
    "At the most basic level an IPython cluster is a set of Python intereters that can be accessed over tcp. Under the hood it works very similar to how Jupyter/IPython work today. When you open a new notebook in the browser a Python process (called a kernel) is started to actually run the code you submit. ipyparallel does the same thing except instead of a single Python kernel, you can start many distributed over many machines.\n",
    "\n",
    "There are three main components to the stack. \n",
    "- Client: A Python process which submits work, Usually this is an IPython session or a Jupyter notebook. \n",
    "- Controller: The central coordinator. Accepts work from the client and passes it to engines, collects results and sends back to the client.\n",
    "- Engine: Roughly equivalent to an IPython kernel. A Python interpreter that communicates with the controller to accept work and submit results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting your first cluster\n",
    "\n",
    "The easiest way to get your hands dirty is to spin up a cluster locally. That is you will run a Client, Controller and Engines all on your local machine. The hardest part of provisioning distributed clusters is making sure all the pieces can talk to each other (as usual the easiest solution to a distributed problem is to make it local).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting your enviornment started\n",
    "\n",
    "Our team are big users of [conda](http://conda.pydata.org/) to help manage our dependecies (Python and beyond). Here is a quick run through to get setup. A combination of pip and virtualenv will also work, but when you start installing packages from the scipy stack we find conda the easist to use.\n",
    "\n",
    "First find your version of Miniconda from [here](http://conda.pydata.org/miniconda.html)\n",
    "\n",
    "If your using linux these commands will work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "wget https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n",
    "bash Miniconda-latest-Linux-x86_64.sh # follow prompts\n",
    "conda update --all\n",
    "# make a new python 3 env named py3\n",
    "conda create -n py3 python=3 ipython ipyparallel ipython-notebook\n",
    "source activate py3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are lower level commands to start and configure Controllers and Engines. The primary command you will use is `ipcluster`. This is a helpful utility to start all the components and configure your local client. By default it uses the `LocalControllerLauncher` and the `LocalEngineSetLauncher` which is exactly what we want to start. \n",
    "\n",
    "Open a terminal install `ipyparallel` and start a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "(py3)➜ ipcluster start --n=4\n",
    "2016-04-11 22:24:15.514 [IPClusterStart] Starting ipcluster with [daemon=False]\n",
    "2016-04-11 22:24:15.515 [IPClusterStart] Creating pid file: /home/vagrant/.ipython/profile_default/pid/ipcluster.pid\n",
    "2016-04-11 22:24:15.515 [IPClusterStart] Starting Controller with LocalControllerLauncher\n",
    "2016-04-11 22:24:16.519 [IPClusterStart] Starting 2 Engines with LocalEngineSetLauncher\n",
    "2016-04-11 22:24:46.633 [IPClusterStart] Engines appear to have started successfully\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can also use the IPython magic shell command. but errors are harder to see and stopping the cluster can be janky.\n",
    "!ipcluster start -n 4 --daemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If started correctly we should now have four engines running on our local machine. Now to actually interact with them. First we need to import the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc.ids # list the ids of the engine the client can communicate with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The client has two primary way to farm out work to the engines. First is a direct view. This is used to apply the same work to all engines. To create a `DirectView` just slice the client.\n",
    "\n",
    "The second way is a `LoadBalancedView` which we will cover later in the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DirectView [0, 1, 2, 3]>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv = rc[:]\n",
    "dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a direct view you can issue a function to execute within the context of that engine's Python process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31183, 31184, 31186, 31188]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_engine_pid():\n",
    "    import os\n",
    "    return os.getpid()\n",
    "    \n",
    "dv.apply_sync(get_engine_pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern is so common that ipyparallel provides a IPython magic function to execute a code cell to all engines: `%%px`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:1]: \u001b[0m31183"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:1]: \u001b[0m31184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:1]: \u001b[0m31186"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:1]: \u001b[0m31188"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "import os\n",
    "os.getpid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is key to notice that the engines are fully running stateful Python interpreters. If you set a varible within `%%px` code block, it will remain there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "foo = 'bar on pid {}'.format(os.getpid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:3]: \u001b[0m'bar on pid 31183'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:3]: \u001b[0m'bar on pid 31184'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:3]: \u001b[0m'bar on pid 31186'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:3]: \u001b[0m'bar on pid 31188'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DirectView` object provides some syntaxic sugar to help distributing data to each engine. First is dictionary style retreival and assignment. First lets retieve the value of `foo` from each engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bar on pid 31183',\n",
       " 'bar on pid 31184',\n",
       " 'bar on pid 31186',\n",
       " 'bar on pid 31188']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv['foo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can overwrite it's its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bar', 'bar', 'bar', 'bar']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv['foo'] = 'bar'\n",
    "dv['foo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many cases where you don't want the same data on each machine, but rather you want to chuck an list and distribute each chunk to an engine. The `DirectView` provides the `.scatter` and the `.gather` methods for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: scatter>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start with a list of ids to work on\n",
    "user_ids = list(range(1000))\n",
    "dv.scatter('user_id_chunk', user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this method completed almost immediately and returned an `AsyncResult`. All the methods we have used up to now have be blocking and syncronous. The `scatter` method is aysnc. To turn this scatter into a blocking call we can chain a `.get()` to the call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.scatter('user_id_chunk', user_ids).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a variable on each engine that holds an equal amount of the original list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "Len 250\n",
      "Max 249\n",
      "[stdout:1] \n",
      "Len 250\n",
      "Max 499\n",
      "[stdout:2] \n",
      "Len 250\n",
      "Max 749\n",
      "[stdout:3] \n",
      "Len 250\n",
      "Max 999\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(\"Len\", len(user_id_chunk))\n",
    "print(\"Max\", max(user_id_chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets apply a simple function to each list. First declare a function within each engine. The `--local` flag also executes the code block in your local client. This is very useful to help debug you code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "def the_most_interesting_transformation_ever(user_id):\n",
    "    \"\"\"\n",
    "    This function is really interesting\n",
    "    \"\"\"\n",
    "    return \"ID:{}\".format(user_id * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ID:3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_most_interesting_transformation_ever(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "transformed_user_ids = list(map(the_most_interesting_transformation_ever, user_id_chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 4 seperate list of transformed ids. We want to stich the disperate lists into one list on our local notebook. `gather` is used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_transformed_user_ids = dv.gather('transformed_user_ids').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['ID:0', 'ID:3', 'ID:6', 'ID:9', 'ID:12', 'ID:15', 'ID:18', 'ID:21', 'ID:24', 'ID:27']\n"
     ]
    }
   ],
   "source": [
    "print(len(all_transformed_user_ids))\n",
    "print(all_transformed_user_ids[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously this example is contrived. The serialization cost of shipping Python objects over the wire to each engine is more expensive than the calculation we preformed. This tradeoff between serialization/transport vs computation cost is central to any decision to use distributed processing. However, there are many highly parallellizable problems where this project can be extremely useful. Some of the main usecases we use ipyparallel for are hyperparamater searches and bulk loading/writing from storage systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoadBalancedView\n",
    "\n",
    "The previous example where you scatter a list, preform a calculation and then gather a result works for lots of problems. One issue that this approach is that each engine has does an identical ammount of work. If the complexity of the process each engine is preforming is variable, this naive scheduling approach can waste processing power and time. Take for example this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px --local\n",
    "import random\n",
    "import time\n",
    "def fake_external_io(url):\n",
    "    # Simulate variable complexity/latency\n",
    "    time.sleep(random.random())\n",
    "    return \"HTML for URL: {}\".format(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 ms, sys: 244 µs, total: 1.26 ms\n",
      "Wall time: 434 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HTML for URL: 1'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fake_external_io(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 ms, sys: 0 ns, total: 2.24 ms\n",
      "Wall time: 835 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HTML for URL: 1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time fake_external_io(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you had a list of urls to scrape and gave each worker an equal share, some workers would finish early and have to sit around doing nothing. A better approach is to assign work to each engine as it finishes. This way the work will be load balanced over the cluster and you will complete your process earlier. ipyparallel provides the `LoadBalancedView` for this exact use case. For this specific problem threading or an async event loop would likely be a better approach to speeding up or scalling out, but suspend your disbelief for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LoadBalancedView None>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lview = rc.load_balanced_view()\n",
    "lview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@lview.parallel()\n",
    "@ipp.require('time', 'random')\n",
    "def p_fake_external_io(url):\n",
    "    # Simulate variable complexity/latency\n",
    "    time.sleep(random.random())\n",
    "    return \"HTML for URL: {}\".format(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used two ipyparallel decorators. First we used `lview.parallel()` to declared this a parallel function. Second, we declared that this function depends on the modules time and random. Now that we have a load balanced function we can compare timings with our naive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = ['foo{}.com'.format(i) for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 ms, sys: 19 ms, total: 47.1 ms\n",
      "Wall time: 46.2 s\n"
     ]
    }
   ],
   "source": [
    "# Naive single threaded\n",
    "%time res = list(map(fake_external_io, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dv.scatter('urls', urls).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 ms, sys: 10.1 ms, total: 21.9 ms\n",
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "# Naive aassignment\n",
    "%time %px results = list(map(fake_external_io, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 277 ms, sys: 53.8 ms, total: 330 ms\n",
      "Wall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "# Load balanced version\n",
    "%time res = p_fake_external_io.map(urls).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a perfect example but you can get the idea. The large the number of inputs to your parallel problem and the more variable the run time of each component process, the more time saving you can get from switching to a load balanced view.\n",
    "\n",
    "This is only scratching the surface of ipyparallel project. I would highly recommend taking a look at the docs. Here is a list of further topics i would look into if you are interested.\n",
    "\n",
    "- support for numpy memmap to allow engine located on a single node to share large arrays\n",
    "- complex dependencies and more specialized scheduling \n",
    "- retry and recovery logic\n",
    "- multiple clients working on the same cluster allowing remote collaborators to share an enviornment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-trival Usecases\n",
    "\n",
    "Our team at Activision largely use ipython clusters for distibuted model training. This project has been vital for hyperparamater searches for our machine learning models. Allowing us to easily parallize these searches beyond one machine has speed up training by many orders of magnitude utilizing hundreds of cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok this is cool but i want more cores!!!\n",
    "\n",
    "The examples so far are a useful introduction to the api and the some features of ipyparallel. Hopefully you are convinced to try out the library. However, deploying a working cluster beyond a single machine introduces some issues. \n",
    "\n",
    "ipyparallel provides support for a range of cluster and batch job management systems such as PBS and WindowsHPC. The full list is provided in the documentation. ipyparallel also provides an SSH based launcher. Given passwordless ssh onto machine you can easily deploy engines and connect them to your controller and client. Also there is a wonderful project [starcluster](http://star.mit.edu/cluster/docs/latest/plugins/ipython.html) that helps spin up machines from cloud providors. \n",
    "\n",
    "These tools are great. If you have access to existing HPC clusters or are planning on deploying dedicated clusters either on your own cold-iron (2016 version of bare-metal) or in the cloud then they meet your needs.\n",
    "\n",
    "However, we are big users of Mesos, Docker and Marathon to manage our clusters and services. Furthermore, even with the existing launchers managing complex dependencies within the engines is a pain. Using Docker to package all dependencies makes deploying heterogenous clusters easier. Targeting our existing cluster management system and simplifying dependencies is a big win for us.\n",
    "\n",
    "With this in mind, we are open sourcing a new ipyparallel launcher that deploys IPython clusters into Mesos using Docker and Marathon. The code lives [here](https://github.com/ActivisionGameScience/ipyparallel-mesos/) and on pypi/conda as `ipyparallel_mesos`.\n",
    "\n",
    "We have two prebuilt Docker images for the [Controller](https://hub.docker.com/r/jdennison/ipyparallel-marathon-controller/) and [Engine](https://hub.docker.com/r/jdennison/ipyparallel-marathon-engine/). These are striped down Docker images. Internally we use conda for almost all our depencies, even inside our Docker containers. Please visit our public [conda recipes](https://github.com/ActivisionGameScience/ags_conda_recipes) and [channel](https://anaconda.org/ActivisionGameScience). However, extending from the `ipyparallel-marathon-engine` image will allow you to easily install your custom dependencies with or with conda.\n",
    "\n",
    "The project is young but hopefully you will find it useful. Please note that this currently targets Python 3. PR's are welcome to support older versions of Python (it's 2016, we can now refer to 2.7 as old). Please open any issues on the github page. Please read the README for the project for more details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources\n",
    "\n",
    "- [ipyparallel_mesos](https://github.com/Activision/ipyparallel-mesos/)\n",
    "- [StarCluster](http://star.mit.edu/cluster/docs/latest/plugins/ipython.html)\n",
    "- [Great IPython Parallel Course - Day 3](https://github.com/jupyter/ngcm-tutorial/tree/master/Day-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
