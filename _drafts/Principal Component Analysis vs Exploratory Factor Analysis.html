---
layout: nb-post
title: "Principal Component Analysis vs Exploratory Factor Analysis"
author: Brian Feldstein
github: BrianFeldstein
tags: [PCA, Exploratory Factor Analysis]
---

<!--put these separators somewhere appropriate:-->



<!--you can set this to false if you want code displayed by default-->
<script>
hide_code=false;
</script>

<!--this is just hacky filler-->
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
<hr>
</p>
</div>
</div>
</div>
<!--back to the good stuff-->

<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<!--if you've decided to show code by default, please reword the following-->
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
NOTE: the code in this notebook can be hidden for readability. To toggle on/off, click <a href="javascript:code_toggle()">here</a>.
</p>
</div>
</div>
</div>


<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Principal-Component-Analysis-Vs-Exploratory-Factor-Analysis">Principal Component Analysis Vs Exploratory Factor Analysis<a class="anchor-link" href="#Principal-Component-Analysis-Vs-Exploratory-Factor-Analysis">&#182;</a></h1><h3 id="Brian-Feldstein"><a href="http://github.com/BrianFeldstein">Brian Feldstein</a><a class="anchor-link" href="#Brian-Feldstein">&#182;</a></h3><p>February 9, 2016</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<!--excerpt.start-->
<p>Recently, exploratory factor analysis (EFA) came up in some work I was doing, and I put some effort into trying to understand its similarities and differences with principal component analysis (PCA).  Finding clear and explicit references on EFA turned out to be hard, but I can recommend taking a look at <a href="https://books.google.com/books?id=VqJ1yhgP1sMC&pg=PA126&lpg=PA126&dq=exploratory+factor+analysis+exact+solution&source=bl&ots=PkpjTZ_NPl&sig=RlJThCcK9jY6eUByPzNC-wtrXLQ&hl=en&sa=X&ved=0ahUKEwjxtfHX8PnJAhWJNSYKHS0aDGo4ChDoAQgrMAQ#v=onepage&q=exploratory%20factor%20analysis%20exact%20solution&f=false">this book</a> and <a href="http://stats.stackexchange.com/questions/123063/is-there-any-good-reason-to-use-pca-instead-of-efa">this Cross Validated question</a>.
Here, I will review both PCA and EFA, and compare and contrast them. These are both techniques which can be used to figure out which combinations of features are important to your data, and for reducing the dimensionality of your feature space.</p>
<!--excerpt.end--><!--more-->
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PCA">PCA<a class="anchor-link" href="#PCA">&#182;</a></h2><p>Suppose we have $p$ features, and $n$ data points, comprising an $n \times p$ matrix $X$.  Without loss of generality,
we may assume that the means for each feature have been subtracted (so that the columns of X each average to 0), and that each feature has also been normalized by its measured standard deviation (so that the standard deviation of each column is 1).</p>
<p>We can form the correlation matrix of X as</p>
<p>$S = \frac{1}{n}X^T X$,</p>
<p>and determine its orthonormal eigenvectors $\overrightarrow{u}_1 .. \overrightarrow{u}_p$, and corresponding positive eigenvalues $d_1 .. d_p$.  These eigenvectors are referred to as "principal components".</p>
<p>We may then project the data onto a subspace spanned by some set of the $\overrightarrow{u}_i$'s with the largest eigenvalues.  The idea is that directions in feature space showing little variation in the data do not contain much useful information, and so can safely be dropped.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For example, let's consider a simple case in which we have two features, which are always equal to each other up to random, normally distributed measurement error terms:</p>
<p>$x_1 = a + e_1$</p>
<p>$x_2 = a + e_2$</p>
<p>Here the $e_i$'s are the error terms, and $a$ is some variable whose value determines the correlated parts of $x_1$ and $x_2$.
We can generate an example data set for this system, with 10,000 data points, as follows:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">S</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">eig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>   
<span class="n">u1</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">u2</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s">&quot;Eigenvalues: &quot;</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s">&quot;Eigenvector 1: &quot;</span><span class="p">,</span> <span class="n">u1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s">&quot;Eigenvector 2: &quot;</span><span class="p">,</span> <span class="n">u2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[[ 1.          0.96050278]
 [ 0.96050278  1.        ]]
Eigenvalues:  [ 1.96050278  0.03949722]
Eigenvector 1:  [ 0.70710678  0.70710678]
Eigenvector 2:  [-0.70710678  0.70710678]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that we have taken the error terms to have standard deviation $.2$, while a is normally distributed with standard deviation $1$.  Below we plot the data set and the two normalized eigenvectors:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQIAAAECCAYAAAAVT9lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that $\overrightarrow{u}_2$, corresponding to the green arrow above, corresponds to a direction in which there is little variation in the data.  We could thus project the data onto the $\overrightarrow{u}_1$ (red) direction without losing too much information.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exploratory-Factor-Analysis">Exploratory Factor Analysis<a class="anchor-link" href="#Exploratory-Factor-Analysis">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In EFA, one makes an <em>ansatz</em> that the features we measure in the data are linearly determined from a smaller set of more fundamental parameters, plus uncorrelated "measurement error" terms.  The data set we generated above for our PCA example happened to follow exactly this sort of ansatz, with 2 measured features $x_i$, and one underlying parameter $a$.</p>
<p>That is, in EFA, we take an ansatz</p>
<p>$x_i = \sum_{k} L_{ik} f_k + e_i$,</p>
<p>where the $f_k$'s are the fundamental underlying parameters, or "factors", and the "loadings" $L$ describe how the measured parameters are determined in terms of the $f_k$'s.  The $e_i$'s are the error terms.  Note that this form for the features is simply a model, and the data scientist looking at a data set has no a priori knowledge what the fundamental parameters are, or even how many there many be.</p>
<p>The goal of EFA is to try to determine what the fundamental parameters- or "factors"- underlying a data set might be. Let us assume there to be $m$ different factors in our model. We may assume that the $f_k$'s are uncorrelated with each other and have unit standard deviation (without loss of generality, since we may always redefine the $f_k$'s to have this property).  We also assume that the $f_k$'s and error terms are uncorrelated.  It then follows from our expression for the $x_i$'s above that if our ansatz is correct, the correlation matrix for the $x_i$'s in the full poulation will take the form</p>
<p>$P = L\ L^T + \psi$,</p>
<p>where $\psi = e\ e^T$ is diagonal since the measurement errors are assumed uncorrelated, and $L$ is a $p\times m$ matrix.</p>
<p>Given a measured data set X with $n$ data points, we may form the sample correlation matrix as $S = \frac{1}{n} X^T X$, and we may attempt to determine $L$ and $\psi$ so that our theoretical correlation matrix $P$ matches $S$ as closely as possible.
More precisely, we may try to minimize the sum of squares of all of the elements of $S-P$.  Note that the diagonal elements of $S-P$ may always be taken to $0$ by appropriately adjusting $\psi$.  Therefore, we would like to find a matrix $L$ such that the sum of squares of the off diagonal elements of $S - L\ L^T$ is as small as possible.</p>
<p>Note that if we can find such a matrix $L$, it is not unique.  We may always take a new matrix $L' = L\ O$ for any $m \times m$ orthogonal matrix O, and $L'\ L'^T$ will remain unchanged.  The columns of $L$ span a subspace of feature space corresponding to the directions of the factors in the model.  Applying an orthogonal transformation $O$ to $L$, simply corresponds to changing our basis for this factor subspace, in such a way that the underlying factors $f$ remain uncorrelated.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us consider again the same data set $X$ we generated above for our PCA example, but instead perform an exploratory factor analysis.  As shown above, our sample correlation matrix has the form</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="nb">print</span><span class="p">(</span><span class="s">&quot;S = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">header</span> <span class="o">=</span> <span class="k">False</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="k">False</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>S = 
 1.000000  0.960503
 0.960503  1.000000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With one factor, a candidate loading matrix is simply</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="nb">print</span><span class="p">(</span><span class="s">&quot;L = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="s">&quot;l1&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s">&quot;l2&quot;</span><span class="p">]]))</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="k">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="k">False</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>L = 
 l1
 l2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So that the off diagonal elements of $S$ may be approximated exactly by setting $l_1 = .962057/l_2$.</p>
<p>We thus see in this example, that one factor, pointing in any direction in feature space, could be thought of as "underlying" the data set (so long as the model also included appropriate measurement errors for the two measured variables in order to also reproduce the diagonal elements of $S$).  Of course, in this case the conclusion is correct: we did indeed generate the data set with a single underlying factor.</p>
<p>However, exactly the same mathematics shows that in fact, in factor analysis, a 2 dimensional feature space may <em>always</em> be represented exactly as coming from a single underlying factor.  This is in clear contrast to PCA, where, if we had set larger errors "e" in generating the data set, then the oval in our above figure would become more spread out and circular.  We would then have concluded that both principal components were needed to describe the data.  In EFA however, we would always find a perfect fit to the off-diagonal correlations with one factor.</p>
<p>The present example also shows that exploratory factor analysis does not lead to unique factors.  When the number of model factors is much smaller than the number of measured features, typically only the orthogonal transformation ambiguity mentioned above is present (in which case the subspace spanned by the factors is fixed).  As in this 2 dimensional example case, however, there can also be further ambiguities.</p>
<p>Factor analysis tries to find a description of the data in terms of a smaller set of underlying factors, but there are in general many such possible descriptions.  What is typically done, is to look for a set of factors with further desirable properties, such as having loading matrices with many elements close to 0.  This sort of structure makes factors easier to "interpret" intuitively in terms of the measured variables.  Perhaps more importantly, if it is actually the case that some of the measured variables are more fundamental than (and determine) the others, it may be possible to use factor analysis to determine which ones they are by looking for the simplest factor structure.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Comparing-PCA-and-EFA">Comparing PCA and EFA<a class="anchor-link" href="#Comparing-PCA-and-EFA">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At this point one might be tempted to conclude that PCA and EFA are essentially unrelated, both in concept and output.</p>
<p>PCA tries to find a set of eigenvectors of the correlation matrix, in order to find directions in the data with the largest variance.  One can then reduce the dimensionality of the data set by projecting onto those eigenvectors with the largest eigenvalues.  In this way one reduces the effective size of the feature space without losing too much information.</p>
<p>Factor Analysis begins with a different point of view; it tries to model correlations between measured variables in the data by writing them as linear combinations of some smaller set of underlying "factors".  EFA does not care about intrinsic variance in a measured variable due to measurement error, and therefore only tries to model the off-diagonal elements of the correlation matrix.</p>
<p><em>However, mathematically, PCA and EFA have a lot more in common with each other than is immediately apparent.</em></p>
<p>Let us arrange the $m$ PCA principal component eigenvectors with $m$ highest eigenvalues into the columns of a matrix $U$.  Let $D$ be the corresponding $m \times m$ diagonal matrix of eigenvalues.  We may define a loading matrix for PCA as $L = U\ D^{1/2}$.  If $m$ was set to its maximum value of $p$ then we would have $S = L\ L^T$.  For smaller $m$, we might wonder how well $L\ L^T$ actually approximates $S$... in fact it provides the best possible approximation to $S$ amongst all $L$'s of the given dimensions, as defined by the sum of squared errors of the matrix elements!  The proof follows quickly from the Eckart-Young theorem (see <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">https://en.wikipedia.org/wiki/Singular_value_decomposition</a>).</p>
<p>Therefore, $P = L\ L^T$ minimizes the sum of squared errors of the matrix $S - P$.  Notice the obvious similarity to factor analysis, where we tried to minimize the same quantity, but with $P = L\ L^T + \psi$ for $\psi$ diagonal.  Thus PCA provides a solution to the factor analysis problem, except that PCA does not ignore intrinsic measurement error on each measured variable in the same way that factor analysis does (and there is no need to assume uncorrelated measurement errors as in factor analysis).</p>
<p>In fact, the loading matrix provided by PCA becomes a better and better approximation to a factor analysis solution as the original number of features $p$ becomes large.  This is because there are $\frac{p^2 - p}{2}$ off-diagonal components of $S$, but only $p$ diagonal components.  Therefore, the fact that factor analysis solutions do not attempt to approximate the diagonal components of $S$ becomes less and less important for larger $p$.</p>
<p>Note that in PCA, the principal component eigenvectors and their eigenvalues may be solved for algebraically. Unfortunately, the same is not true for EFA: the loading matrices $L$ can only be determined numerically.  Moreover, in EFA, given the form of the model $x_i = \sum_{k} L_{ik} f_k + e_i$, and some set of factor loadings $L$ and a set of measurements $x_i$, there is in general no unique way to determine what specific values of the $f_k$ were behind the measurements.  To estimate the $f_k$, various procedures may be adopted, such as choosing the $f_k$ which minimize the required measurement errors $e_i$.  The resulting factor scores may then be used as a "reduced" set of feature values if desired, just as PCA may be used for dimensionality reduction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Principal Component Analysis and Exploratory Factor analysis are both methods which may be used to reduce the dimensionality of data sets.  Philosophically they are very different: PCA tries to write all variables in terms of a smaller set of features which allows for a maximum amount of variance to be retained in the data.  EFA tries to find a set of features which allow for understanding as much of the <em>correlations</em> between measured variables as possible.  However, EFA does not attempt to reproduce variance which is due to "measurement error" only affecting measured variables individually.</p>
<p>In PCA the principal component directions are taken to be eigenvectors of the correlation matrix, while an analogous requirement is not used in EFA.  In EFA one rather tries to find a set of underlying features which is related in a simple way to the original measured variables, although there is generally not a unique solution.  Mathematically, a set of principle components from PCA will give an approximate solution for the EFA problem when the number of measured variables is large.</p>

</div>
</div>
</div>
</div>
</div>
