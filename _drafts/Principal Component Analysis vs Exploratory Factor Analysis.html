---
layout: nb-post
title: "Principal Component Analysis vs Exploratory Factor Analysis"
author: Brian Feldstein
github: BrianFeldstein	
tags: [PCA]
---

<!--put these separators somewhere appropriate:-->



<!--this is just hacky filler-->
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
<hr>
</p>
</div>
</div>
</div>
<!--back to the good stuff-->

<div tabindex="-1" id="notebook" class="border-box-sizing">
<div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>
NOTE: the code in this notebook is hidden for better readability. To toggle on/off, click <a href="javascript:code_toggle()">here</a>.
</p>
</div>
</div>
</div>


<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Principal-Component-Analysis-Vs-Exploratory-Factor-Analysis">Principal Component Analysis Vs Exploratory Factor Analysis<a class="anchor-link" href="#Principal-Component-Analysis-Vs-Exploratory-Factor-Analysis">&#182;</a></h1><h3 id="Brian-Feldstein"><a href="http://github.com/BrianFeldstein">Brian Feldstein</a><a class="anchor-link" href="#Brian-Feldstein">&#182;</a></h3><p>February 5, 2016</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<!--excerpt.start-->
<p>In this blog post we will discuss the similarities and differences between principal component analysis (PCA) and exploratory factor analysis (EFA).  These are both techniques which can be used to figure out which combinations of features are important to your data, and for reducing the dimensionality of your feature space.</p>
<!--excerpt.end--><!--more-->
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="PCA">PCA<a class="anchor-link" href="#PCA">&#182;</a></h2><p>Suppose we have $p$ features, and $n$ data points, comprising an $n \times p$ matrix $X$.  Without loss of generality,
we may assume that the means for each feature have been subtracted (so that the columns of X each average to 0), and that each feature has also been normalized by its measured standard deviation (so that the standard deviation of each column is 1).</p>
<p>We can form the correlation matrix of X as</p>
<p>$S = \frac{1}{n}X^T X$,</p>
<p>and determine its orthonormal eigenvectors $\overrightarrow{u}_1 .. \overrightarrow{u}_p$, and corresponding positive eigenvalues $d_1 .. d_n$.  These eigenvectors are referred to as "principal components".</p>
<p>We may then project the data onto a subspace spanned by some set of the $\overrightarrow{u}_i$'s with the largest eigenvalues.  The idea is that directions in feature space showing little variation in the data do not contain much useful information, and so can safely be dropped.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For example, let's consider a simple case in which we have two features, which are always equal to each other up to random, normally distributed measurement error terms:</p>
<p>$x_1 = a + e_1$</p>
<p>$x_2 = a + e_2$</p>
<p>Here the $e_i$'s are the error terms, and $a$ is some variable whose value determines the correlated parts of $x_1$ and $x_2$.
We can generate an example data set for this system, with 10,000 data points, as follows:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span> <span class="o">+</span> <span class="n">e</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">S</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">eig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>   
<span class="n">u1</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">u2</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span> <span class="n">idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">eig</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s">&quot;Eigenvalues: &quot;</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s">&quot;Eigenvector 1: &quot;</span><span class="p">,</span> <span class="n">u1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s">&quot;Eigenvector 2: &quot;</span><span class="p">,</span> <span class="n">u2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>[[ 1.          0.96259933]
 [ 0.96259933  1.        ]]
Eigenvalues:  [ 1.96259933  0.03740067]
Eigenvector 1:  [ 0.70710678  0.70710678]
Eigenvector 2:  [-0.70710678  0.70710678]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note that we have taken the error terms to have standard deviation $.2$, while a is normally distributed with standard deviation $1$.  Below we plot the data set and the two normalized eigenvectors:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">axes</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u1</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">u2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">u2</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;g&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">


<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.patches.FancyArrow at 0x4926ac8&gt;</pre>
</div>

</div>

<div class="output_area">


<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAQIAAAECCAYAAAAVT9lQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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=
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that $\overrightarrow{u}_2$, corresponding to the green arrow above, corresponds to a direction in which there is little variation in the data.  We could thus project the data onto the $\overrightarrow{u}_1$ (red) direction without losing too much information.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Exploratory-Factor-Analysis">Exploratory Factor Analysis<a class="anchor-link" href="#Exploratory-Factor-Analysis">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In EFA, one makes an <em>ansatz</em> that the features we measure in the data are linearly determined from a smaller set of more fundamental parameters, plus uncorrelated "measurement error" terms.  The data set we generated above for our PCA example happened to follow exactly this sort of ansatz, with 2 measured features $x_i$, and one underlying parameter $a$.</p>
<p>That is, in EFA, we take an ansatz</p>
<p>$x_i = \sum_{k} L_{ik} f_k + e_i$,</p>
<p>where the $f_k$'s are the fundamental underlying parameters, or "factors", and the "loadings" $L$ describe how the measured parameters are determined in terms of the $f_k$'s.  The $e_i$'s are the error terms.  Note that this form for the features is simply a model, and the data scientist looking at a data set has no a priori knowledge what the fundamental parameters are, or even how many there many be.</p>
<p>The goal of EFA is to try to determine what the fundamental parameters- or "factors"- underlying a data set might be. Let us assume there to be $m$ different factors in our model. We may assume that the $f_k$'s are uncorrelated with each other and have unit standard deviation (without loss of generality, since we may always redefine the $f_k$'s to have this property).  We also assume that the $f_k$'s and error terms are uncorrelated.  It then follows from our expression for the $x_i$'s above that if our ansatz is correct, the correlation matrix for the $x_i$'s in the full poulation will take the form</p>
<p>$P = L\ L^T + \psi$,</p>
<p>where $\psi = e\ e^T$ is diagonal since the measurement errors are assumed uncorrelated, and $L$ is a $p\times m$ matrix.</p>
<p>Given a measured data set X with $n$ data points, we may form the sample correlation matrix as $S = \frac{1}{n} X^T X$, and we may attempt to determine $L$ and $\psi$ so that our theoretical correlation matrix $P$ matches $S$ as closely as possible.
More precisely, we may try to minimize the sum of squares of all of the elements of $S-P$.  Note that the diagonal elements of $S-P$ may always be taken to $0$ by appropriately adjusting $\psi$.  Therefore, we would like to find a matrix $L$ such that the sum of squares of the off diagonal elements of $S - L\ L^T$ is as small as possible.</p>
<p>Note that if we can find such a matrix $L$, it is not unique.  We may always take a new matrix $L' = L\ O$ for any $m \times m$ orthogonal matrix O, and $L'\ L'^T$ will remain unchanged.  The columns of $L$ span a subspace of feature space corresponding to the directions of the factors in the model.  Applying an orthogonal transformation $O$ to $L$, simply corresponds to changing our basis for this factor subspace, in such a way that the underlying factors $f$ remain uncorrelated.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let us consider again the same data set $X$ we generated above for our PCA example, but instead perform an exploratory factor analysis.  As shown above, our sample correlation matrix has the form</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="nb">print</span><span class="p">(</span><span class="s">&quot;S = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">S</span><span class="p">)</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="k">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="k">False</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>S = 
 1.000000  0.962599
 0.962599  1.000000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With one factor, a candidate loading matrix is simply</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div></div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span class="nb">print</span><span class="p">(</span><span class="s">&quot;L = &quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="s">&quot;l1&quot;</span><span class="p">],[</span><span class="s">&quot;l2&quot;</span><span class="p">]]))</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">header</span><span class="o">=</span><span class="k">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="k">False</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="output_subarea output_stream output_stdout output_text">
<pre>L = 
 l1
 l2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So that the off diagonal elements of $S$ may be approximated exactly by setting $l_1 = .962057/l_2$.</p>
<p>We thus see in this example, that one factor, pointing in any direction in feature space, could be thought of as "underlying" the data set (so long as the model also included appropriate measurement errors for the two measured variables in order to also reproduce the diagonal elements of $S$).  Of course, in this case the conclusion is correct: we did indeed generate the data set with a single underlying factor.</p>
<p>However, exactly the same mathematics shows that in fact, in factor analysis, a 2 dimensional feature space may <em>always</em> be represented exactly as coming from a single underlying factor.  This is in clear contrast to PCA, where, if we had set larger errors "e" in generating the data set, then the oval in our above figure would become more spread out and circular.  We would then have concluded that both principal components were needed to describe the data.  In EFA however, we would always find a perfect fit with one factor.</p>
<p>The present example also shows that exploratory factor analysis does not lead to unique factors.  When the number of model factors is much smaller than the number of measured features, typically only the orthogonal transformation ambiguity mentioned above is present (in which case the subspace spanned by the factors is fixed).  As in this 2 dimensional example case, however, there can also be further ambiguities.</p>
<p>Factor analysis tries to find a description of the data in terms of a smaller set of underlying factors, but there are in general many such possible descriptions.  What is typically done, is to look for a set of factors with further desirable properties, such as having loading matrices with many elements close to 0.  This sort of structure makes factors easier to "interpret" intuitively in terms of the measured variables.  Perhaps more importantly, if it is actually the case that some of the measured variables are more fundamental than (and determine) the others, it may be possible to use factor analysis to determine which ones they are by looking for the simplest factor structure.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Comparing-PCA-and-EFA">Comparing PCA and EFA<a class="anchor-link" href="#Comparing-PCA-and-EFA">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At this point one might be tempted to conclude that PCA and EFA are essentially unrelated, both in concept and output.</p>
<p>PCA tries to find a set of eigenvectors of the correlation matrix, in order to find directions in the data with the largest variance.  One can then reduce the dimensionality of the data set by projecting onto those eigenvectors with the largest eigenvalues.  In this way one reduces the effective size of the feature space without losing too much information.</p>
<p>Factor Analysis begins with a different point of view; it tries to model correlations between measured variables in the data by writing them as linear combinations of some smaller set of underlying "factors".  EFA does not care about intrinsic variance in a measured variable due to measurement error, and therefore only tries to model the off-diagonal elements of the correlation matrix.</p>
<p><em>However, mathematically, PCA and EFA have a lot more in common with each other than is immediately apparent.</em></p>
<p>Let us arrange the $m$ PCA principal component eigenvectors with $m$ highest eigenvalues into the columns of a matrix $U$.  Let $D$ be the corresponding $m \times m$ matrix of eigenvalues.  We may define a loading matrix for PCA as $L = U\ D^{1/2}$.  If $m$ was set to its maximum value of $p$ then we would have $S = L\ L^T$.  For smaller $m$, we might wonder how well $L\ L^T$ actually approximates $S$... in fact it provides the best possible approximation to $S$ amongst all $L$'s of the given dimensions, as defined by the sum of squared errors of the matrix elements!  The proof follows quickly from the Eckart-Young theorem (see <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">https://en.wikipedia.org/wiki/Singular_value_decomposition</a>).</p>
<p>Therefore, $P = L\ L^T$ minimizes the sum of squared errors of the matrix $S - P$.  Notice the obvious similarity to factor analysis, where we tried to minimize the same quantity, but with $P = L\ L^T + \psi$ for $\psi$ diagonal.  Thus PCA provides a solution to the factor analysis problem, except that PCA does not ignore intrinsic measurement error on each measured variable in the same way that factor analysis does (and there is no need to assume uncorrelated measurement errors as in factor analysis).</p>
<p>In fact, the loading matrix provided by PCA becomes a better and better approximation to a factor analysis solution as the original number of features $p$ becomes large.  This is because there are $\frac{p^2 - p}{2}$ diagonal components of $S$, but only $p$ diagonal components.  Therefore, the fact that factor analysis solutions do not attempt to approximate the diagonal components of $S$ becomes less and less important for larger $p$.</p>
<p>Note that in PCA, the principal component eigenvectors and their eigenvalues may be solved for algebraically. Unfortunately, the same is not true for EFA: the loading matrices $L$ can only be determined numerically.  Moreover, in EFA, given the form of the model $x_i = \sum_{k} L_{ik} f_k + e_i$, and some set of factor loadings $L$ and a set of measurements $x_i$, there is in general no unique way to determine what the underlying factor scores $f_k$ were behind the measurements.  To estimate the $f_k$, various procedures may be adopted, such as choosing the $f_k$ which minimize the required measurement errors $e_i$.  The resulting factor scores may then be used as a "reduced" set of feature values if desired, just as PCA may be used for dimensionality reduction.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div>
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Principal Component Analysis and Exploratory Factor analysis are both methods which may be used to reduce the dimensionality of data sets.  Philosophically they are very different: PCA tries to write all variables in terms of a smaller set of features which allows for a maximum amount of variance to be retained in the data.  EFA tries to find a set of features which allow for understanding as much of the <em>correlations</em> between measured variables as possible.  However, EFA does not attempt to reproduce variance which is due to "measurement error" only affecting measured variables individually.</p>
<p>In PCA the principal component directions are taken to be eigenvectors of the correlation matrix, while an analogous requirement is not used in EFA.  In EFA one rather tries to find a set of underlying features which is related in a simple way to the original measured variables, although there is generally not a unique solution.  Mathematically, a set of principle components from PCA will give an approximate solution for the EFA problem when the number of measured variables is large.</p>

</div>
</div>
</div>
</div>
</div>
